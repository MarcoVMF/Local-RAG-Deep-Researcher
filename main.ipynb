{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import json \n",
    "import getpass\n",
    "from typing import List\n",
    "from dataclasses import field\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.schema import Document"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25218c6f640e1c88"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8dea402419271a1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "        \n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f7b9e559da91f3b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"local-llama-deep-researcher-v2\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3db763286afa5b4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"deepseek\": \"deepseek-r1:8b\",\n",
    "    \"llama\": \"llama3.2:3b\"\n",
    "}\n",
    "\n",
    "model = models['llama']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbd00b768f5410e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    previous_queries: List[str] = field(default=None)\n",
    "    query: str = field(default=None)\n",
    "    topic: str = field(default=None)\n",
    "    documents: List[str] = field(default=None)\n",
    "    summary: str = field(default=None)\n",
    "    search_loop_step: int = field(default=0)\n",
    "    url_sources: List[str] = field(default=None)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "838a292a88254de2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_query(state):\n",
    "    print(\"-- Generating query --\")\n",
    "    prompt = \"\"\" Your goal is to generate targeted web search query.\n",
    "        The query will gather information related to a specific topic.\n",
    "        Topic: {topic}\n",
    "        Return your query as a JSON object:\n",
    "        {{\n",
    "            \"query\": \"string\",\n",
    "            \"aspect\": \"string\",\n",
    "            \"rationale\": \"string\"\n",
    "        }}\"\"\"\n",
    "    \n",
    "    prompt_formatted = prompt.format(topic=state['topic'])\n",
    "    llm_json_mode = ChatOllama(model=model, temperature=0, format='json')\n",
    "    response = llm_json_mode.invoke(prompt_formatted)\n",
    "    query = json.loads(response.content)\n",
    "    \n",
    "    return {\"query\": query['query']} \n",
    "    \n",
    "\n",
    "def web_research(state:GraphState):\n",
    "    print(\"-- Starting Web Research --\")\n",
    "    web_search_tool = TavilySearchResults(k=3)\n",
    "    \n",
    "    documents = state.get(\"documents\", [])\n",
    "    url_sources = state.get('url_sources', [])\n",
    "    \n",
    "    \n",
    "    \n",
    "    docs = web_search_tool.invoke({'query': state['query']})\n",
    "    web_results = \"\\n\".join(d['content'] for d in docs)\n",
    "    web_results = Document(page_content=web_results)      \n",
    "    documents.append(web_results)\n",
    "    for d in docs:\n",
    "        url_sources.append(d['url'])\n",
    "    \n",
    "    return {'documents': documents, 'url_sources': url_sources}\n",
    "    \n",
    "\n",
    "def summarize_source(state:GraphState):\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    Your goal is to generate a high-quality summary of the web search results.\n",
    "\n",
    "    When EXTENDING an existing summary:\n",
    "    1. Seamlessly integrate new information without repeating what's already covered\n",
    "    2. Maintain consistency with the existing content's style and depth\n",
    "    3. Only add new, non-redundant information\n",
    "    4. Ensure smooth transitions between existing and new content\n",
    "    \n",
    "    When creating a NEW summary:\n",
    "    1. Highlight the most relevant information from each source\n",
    "    2. Provide a concise overview of the key points related to the report topic\n",
    "    3. Emphasize significant findings or insights\n",
    "    4. Ensure a coherent flow of information\n",
    "    \n",
    "    CRITICAL REQUIREMENTS:\n",
    "    - Start IMMEDIATELY with the summary content - no introductions or meta-commentary\n",
    "    - DO NOT include ANY of the following:\n",
    "      * Phrases about your thought process (\"Let me start by...\", \"I should...\", \"I'll...\")\n",
    "      * Explanations of what you're going to do\n",
    "      * Statements about understanding or analyzing the sources\n",
    "      * Mentions of summary extension or integration\n",
    "    - Focus ONLY on factual, objective information\n",
    "    - Maintain a consistent technical depth\n",
    "    - Avoid redundancy and repetition\n",
    "    - DO NOT use phrases like \"based on the new results\" or \"according to additional sources\"\n",
    "    - DO NOT add a References or Works Cited section\n",
    "    - DO NOT use any XML-style tags like <think> or <answer>\n",
    "    - Begin directly with the summary text without any tags, prefixes, or meta-commentary\n",
    "    \n",
    "    Summary:\n",
    "    {summary}\n",
    "    \n",
    "    Documents retrieved from internet:\n",
    "    {document}\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = format_docs(state['documents'])\n",
    "    summary = state.get('summary', 'Do not exist an summary, you need to create one')\n",
    "    prompt_formatted = prompt.format(document=doc, summary=summary)\n",
    "    \n",
    "    \n",
    "    llm = ChatOllama(model=model, temperature=0)\n",
    "    summary = llm.invoke(prompt_formatted)\n",
    "    return {\"summary\" : summary, \"existing_summary\": True}\n",
    "\n",
    "def reflect_on_summary(state:GraphState):\n",
    "    \n",
    "    prompt = \"\"\"\n",
    "    You are a research optimization assistant. Analyze the current summary and generate one improved search query to fill information gaps the summary's topic is: {topic}\n",
    "    \n",
    "    Your tasks:\n",
    "        1. Identify knowledge gaps or areas that need deeper exploration\n",
    "        2. Generate a follow-up query that would help expand your understanding\n",
    "        3. Focus on technical details, implementation specifics, or emerging trends that weren't fully covered\n",
    "\n",
    "    Ensure the follow-up question is self-contained and includes necessary context for web search.\n",
    "    \n",
    "    Summary:\n",
    "    {summary}\n",
    "    \n",
    "\n",
    "    Return your query as a JSON object:\n",
    "        {{\n",
    "            \"new_query\": \"string\",\n",
    "            \"aspect\": \"string\",\n",
    "            \"rationale\": \"string\"\n",
    "        }}\"\"\"\n",
    "    \n",
    "    summary = state['summary']\n",
    "    topic = state['topic']\n",
    "    prompt_formatted = prompt.format(summary=summary,\n",
    "                                     topic=topic)\n",
    "    \n",
    "    previous_queries = state.get('previous_queries', [])\n",
    "    current_query = state.get('query', None)\n",
    "    previous_queries.append(current_query)\n",
    "    \n",
    "    llm_json_mode = ChatOllama(model=model, temperature=0, format='json')\n",
    "    response = llm_json_mode.invoke(prompt_formatted)\n",
    "    query = json.loads(response.content)\n",
    "    query = query['new_query']\n",
    "    search_loop_step = state.get('search_loop_step', 0)\n",
    "    \n",
    "    return {'query': query, 'search_loop_step': search_loop_step+1, 'previous_queries': previous_queries}\n",
    "\n",
    "    \n",
    "    \n",
    "def finalize_summary(state:GraphState):\n",
    "    print(\"--- CURRENT QUERY ---\")\n",
    "    print(state['query'])\n",
    "    print(\"--- PREVIOUS QUERIES ---\")\n",
    "    print(state['previous_queries'])\n",
    "    print(\"--- TOPIC ---\")\n",
    "    print(state['topic'])\n",
    "    print(\"--- SUMMARY ---\")\n",
    "    print(state['summary'])\n",
    "    print(\"--- URL SOURCES ---\")\n",
    "    print(state['url_sources'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ceab3121e733c9a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "\n",
    "workflow.add_node(\"generate_query\", generate_query)\n",
    "workflow.add_node(\"web_research\", web_research)\n",
    "workflow.add_node(\"summarize_source\", summarize_source)\n",
    "workflow.add_node(\"reflect_on_summary\", reflect_on_summary)\n",
    "workflow.add_node(\"finalize_summary\", finalize_summary)\n",
    "\n",
    "def loop_condition(state: GraphState):\n",
    "    return state['search_loop_step'] < 3\n",
    "\n",
    "workflow.set_entry_point(\"generate_query\")\n",
    "\n",
    "workflow.add_edge(\"generate_query\", \"web_research\")  \n",
    "workflow.add_edge(\"web_research\", \"summarize_source\")\n",
    "workflow.add_edge(\"summarize_source\", \"reflect_on_summary\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"reflect_on_summary\",\n",
    "    loop_condition,\n",
    "    {\n",
    "        True: \"web_research\",  \n",
    "        False: \"finalize_summary\",  \n",
    "    },\n",
    ")\n",
    "\n",
    "graph = workflow.compile()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50b9bb01e83b3d93"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs = {\"topic\": \"How does Chain of Thoughts works?\"}\n",
    "\n",
    "for event in graph.stream(inputs, stream_mode=\"values\"):\n",
    "    print(event)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff3f9b0aea75f679"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cd2e84dcab2e95b0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
